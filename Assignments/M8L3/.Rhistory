qplot(created_at_month, data = L3Q1_data) + geom_boxplot()
summary(L3Q1_data$country)
qplot(location, data = L3Q1_data,geom = "bar")
summary(L3Q1_data$location)
qplot(2, location, data = L3Q1_data) + geom_boxplot()
L3Q1_data$friends_count
qplot(friend_count, data = L3Q1_data,geom = "bar")
qplot(friends_count, data = L3Q1_data) + geom_boxplot()
qplot(friends_count, data = L3Q1_data,geom="boxplot")
qplot(1, friends_count, data = L3Q1_data,geom="boxplot")
qplot(favourited_count, data = L3Q1_data, xlim = c(0,10^4), geom = "density")
L3Q1_data$dob_month
max(L3Q1_data$dob_month)
L3Q1_data$gender
L3Q1_data$education
L3Q1_data$mobile_favourites_count
education
L3Q1_data$education
qqnorm(L3Q1_data$education)
qqline(L3Q1_data$education)
L3Q1_data$experience
L3Q1_data$race
?read.table
pairs(data)
?melt
?melt
?reshape2
library("reshape2", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
?melt
?qplot
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
?qplot
?geom
summary(lm(wage~height+education, data=trim_data_mul))
trim_data_mul <- data.frame(trim_data$wage, trim_data$height, as.numeric(trim_data$race), trim_data$age, trim_data$education, trim_data$experience )
require("ggplot2")
require("reshape2")
data_url <-'http://nikbearbrown.com/YouTube/MachineLearning/M01/M01_quasi_twitter.csv'
twitter_data<- read.csv( url(data_url) )
trim_data = subset(twitter_data, gender =="female" | gender == "male")
library(lmtest)
library(nortest)
trim_data_mul <- data.frame(trim_data$wage, trim_data$height, as.numeric(trim_data$race), trim_data$age, trim_data$education, trim_data$experience )
summary(lm(wage~height+education, data=trim_data_mul))
names(trim_data_mul) <- c("wage","height","race","age", "education", "experience")
summary(lm(wage~height+education, data=trim_data_mul))
m_mul_wage_final
m_mul_wage_final <- lm(wage ~ height+education , data= trim_data_mul)
summary(m_mul_wage_final )
anova(m_mul_wage_final )
plot(wage, data=twitter_data)
plot(twitter_data$wage, data=twitter_data)
plot(twitter_data$wage, data=twitter_data, geom = "density")
plot(twitter_data$wage, data=twitter_data, geom = "boxplot")
qplot(twitter_data$wage, data=twitter_data, geom = "boxplot")
qplot(twitter_data$wage, geom = "boxplot")
qplot( twitter_data$gender, twitter_data$wage) + geom_boxplot() + ylim(0,1000)
qplot( twitter_data$gender, twitter_data$wage) + geom_boxplot()
qplot( trim_data$gender, trim_data$wage) + geom_boxplot()
lm(as.numeric(trim_data%gender),trim_data$wage)
lm(as.numeric(trim_data%gender)~trim_data$wage)
lm(as.numeric(trim_data$gender)~trim_data$wage)
summary(m_wage_gender)
m_wage_gender <- lm(as.numeric(trim_data$gender)~trim_data$wage)
summary(m_wage_gender)
plot(trim_data$gender~trim_data$wage)
qplot(trim_data$gender~trim_data$wage)
qplot(trim_data$gender~trim_data$wage,geom="dotplot")
dotplot(trim_data$gender~trim_data$wage)
plot(trim_data$gender~trim_data$wage)
qplot(wage, gender, data = trim_data )
summary(m_wage_gender)
anova(m_wage_gender)
plot(m_wage_gender)
qplot( trim_data$gender, trim_data$height) + geom_boxplot()
m_height_gender <- lm(as.numeric(trim_data$gender)~trim_data$height)
summary(m_height_gender)
anova(m_height_gender)
qplot(height, gender, data = trim_data )+stat_smooth(method="lm",se=FALSE)
plot(m_height_gender)
bptest(m_height_gender, studentize = F)
lillie.test(resid(m_height_gender))
m_height_gender <- lm(as.numeric(trim_data$gender)~trim_data$height)
summary(m_height_gender)
url = 'http://www.bls.gov/cew/data/files/2014/csv/2014_annual_singlefile.zip'
temp = tempfile()
download.file(url,temp, mode = "w")
data <- read.csv(unzip(temp), header = TRUE)
unlink(temp)
keep_data = data.frame(data[,9:15],data[,17:23], data[,25:38])
data.keep = data.frame(keep_data[,c(1:5,7:12, 14:24, 27,28)])
names(data.keep)
BLS.fit.A <- princomp(formula = ~., data.keep, cor = TRUE, na.action = na.exclude)
BLS.fit.A
dim(data)
library('ggbiplot')
ggbiplot(BLS.fit.A, labers = rownames(data.keep), obs.scale = 1, var.scale = 1)
data(wine)
wine.class
ggbiplot(BLS.fit.A, labers = rownames(data.keep), obs.scale = 1, var.scale = 1, xlim = c(-500, 10), ylim = c(-10,500))
library('ggplot')
library('ggplots')
library('ggplot2')
ggbiplot(BLS.fit.A, labers = rownames(data.keep), obs.scale = 1, var.scale = 1) + xlim(-500, 10) + ylim (-10,500)
?bpCent
BLS.fit.A
names(BLS.fit.A)
BLS.fit.A$scores
fit.A.components = BLS.fit.A$scores
fit.A.components
fit.A.components[,1]
head(fit.A.components[,1])
ggbiplot(fit.A.components[,1],fit.A.components[,2], labers = rownames(data.keep), obs.scale = 1, var.scale = 1, xlim = c(-500, 10), ylim = c(-10,500))
wine.pca <- prcomp(wine, scale. = TRUE)
ggbiplot(wine.pca, obs.scale = 1, var.scale = 1,
groups = wine.class, ellipse = TRUE, circle = TRUE) +
scale_color_discrete(name = '') +
theme(legend.direction = 'horizontal', legend.position = 'top')
ggbiplot(wine.pca, obs.scale = 1, var.scale = 1,
groups = wine.class, ellipse = TRUE, circle = TRUE)
ggbiplot(wine.pca, obs.scale = 1, var.scale = 1,
groups = wine.class, ellipse = TRUE, circle = TRUE)
ggbiplot(wine.pca, obs.scale = 1, var.scale = 1)
biplot(wine.pca)
biplot(BLS.fit.A)
install.packages("cluster")
install.packages("amap")
install.packages("useful")
?matrix
i= 1
for (i in 5){
print i
}
for (i in 5){
print i
}
i <- 1
for (i in 5){
print i
for (i in 5){
i
}
for (i in 5){
print(i)
}
for (i in c(1:5)){
print(i)
}
for (i in range(1:5)){
print(i)
}
for (i in (1:5)){
print(i)
}
data_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data'
cancer <- read.table(url(data_url), sep = ',')
dim(cancer)
qplot(x = tumor_size , data = cancer, geom = "density", group = Outcome, color = Outcome)
install.packages('mclust')
install.packages("class")
?knn
library("class")
?knn
sqrt(569)
sqrt(569)*2
?lapply
k <- 7
install.packages("C50")
install.packages("gmodels")
install.packages("rpart")
install.packages("RColorBrewer")
install.packages("tree")
install.packages("party")
data_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'
cancer_data <- read.table(url(data_url), sep = ',')
head(cancer_data)
cancer_data[2]
?runif
rnif(3)
runif(3)
runif(5)
order(runif(3))
?order
a <- c(1,10,2)
order(a)
?rpart
?printcp
install.packages("e1071")
install.packages("kernlab")
?svm
library("e1071")
?svm
?plot
y<-c(rep(-1,10),rep(1,10))
?matrix
y
?plot
?lm
?glm
?glm
?rmse
?svm
linear.tune.out <- tune(svm, cancer.train.target~., data=cancer.train, kernel="linear", ranges= list(cost=c(0.0001, 0.01, 0.1, 1,5,10,100)))
linear.tune.out <- tune(svm, cancer.train.target, cancer.train, kernel="linear", ranges= list(cost=2^(-2:4)))
library("MASS")
?lda
?predict
cancer.normalized.lda.pred <- predict(cancer.normalized.lda, newdata = cancer.normalized.test)
pca
?pca
?princomp
?kmeans
library("RTextTools")
library("e1071")
library("qdap")
library("tm")
tweets.url <- "http://nikbearbrown.com/YouTube/MachineLearning/M08/ML.Tweets.csv"
tweets.new.url <- "http://nikbearbrown.com/YouTube/MachineLearning/M08/ML.Tweets.New.csv"
tweets <- read.csv(url(tweets.url), header=F)
tweets.new <- read.csv(url(tweets.new.url), header=F)
has_tag <- function(x){
grepl("#[_a-zA-Z1-9]+",x)
}
tag_result <- apply(tweets, 1, has_tag)
tweets_ht <- tweets[which(tag_result),]
clean_sub <- function(x){
x <- tolower((x))
#remove all puncuation except #
gsub("(?!#)[[:punct:]]", "",perl=T,
#remove all control characters
gsub("[[:cntrl:]]", "",
#remove all digit
gsub("\\d+","",
#remove the users
gsub("@([_a-zA-Z1-9]+)","",
#remove all url
gsub("(https?://[^ ]+)","",x)))))
}
tweets_clean <- apply(as.data.frame(tweets_ht), 1, clean_sub)
hashtags_re <- function(line){
m <- gregexpr("#([_a-zA-Z1-9]+)", line)
return(regmatches(line,m))
}
tweets_clean <- as.data.frame(tweets_clean)
result_hashtag <- apply(tweets_clean, 1, hashtags_re)
hashtags.corpus <- Corpus(VectorSource(result_hashtag))
inspect(hashtags.corpus)
hashtags.tdm <- TermDocumentMatrix(hashtags.corpus, control=list(removePunctuation=T))
m<- as.matrix(hashtags.tdm)
term_nums <- rowSums(m)
tweets_sum <- rowSums(m!=0)
#calculate tf of each hashtag
tf <- term_nums/tweets_sum
#calculate idf of each hashtag
doc_num <- nrow(tweets_clean)
idf <-log(doc_num/tweets_sum)
#to get each hashtag tf-idf
tf_idf <- tf*idf
hash_sorted <- sort(tf_idf, decreasing = T)
library("cluster")
hashtags.500.corpus <- Corpus(VectorSource(result_hashtag))
inspect(hashtags.500.corpus)
hashtags.500.tdm <- TermDocumentMatrix(hashtags.500.corpus, control=list(removePunctuation=T))
tweets.matrix <- as.matrix(hashtags.500.tdm)
distMatrix <- dist(scale(t(tweets.matrix)[1:2000,1:500]))
tweets.fit <- hclust(distMatrix,method = "ward.D")
groups <- cutree(tweets.fit, k=5)
groups.1 <- groups[groups==1]
groups.2 <- groups[groups==2]
groups.3 <- groups[groups==3]
groups.4 <- groups[groups==4]
groups.5 <- groups[groups==5]
head(groups.1)
head(groups.2)
head(groups.3)
head(groups.4)
groups.5
tweets_clean[1:2000]
train_tweets <- data.framet(tweets = tweets_clean[1:2000,], )
tweets_clean[1:2000,]
train_tweets <- data.framet(tweets = tweets_clean[1:2000,], cluster = groups)
train_tweets <- data.frame(tweets = tweets_clean[1:2000,], cluster = groups)
train_tweets <- data.frame(tweets = tweets_clean[1:2000,], cluster = groups)
new_tag_result <- apply(tweets.new, 1, has_tag)
tweets_new_ht <- tweets.new[which(new_tag_result),]
tweets_new_clean <- apply(as.data.frame(tweets_new_ht), 1, clean_sub)
head(tweets_new_clean)
head(train_tweets)
matrix <- create_matrix(train_tweets[,1], language = "english", removeStopwords = T, stemWords = F,tm::weightTfIdf)
mat <- as.matrix(matrix)
head(mat)
classifier <- naiveBayes(mat, as.factor(train_tweets[,2]))
predicted <- predict(classifier,tweets_new_clean)
predicted <- predict(classifier,tweets_new_clean[1:1000])
predicted <- predict(classifier,tweets_new_clean[1:1000,])
tweets_new_clean[1:1000,]
tweets_new_clean[1:1000]
predicted <- predict(classifier,tweets_new_clean[1:100])
predicted
predicted <- predict(classifier,tweets_new_clean[1:500])
predicted
predicted <- predict(classifier,tweets_new_clean[1:1000])
head(train_tweets)
head(train_tweets,300)
groups.1
length(groups.1)
groups <- cutree(tweets.fit, k=10)
predicted <- predict(classifier,tweets_new_clean[1:1000])
train_tweets <- data.frame(tweets = tweets_clean[1:2000,], cluster = groups)
matrix <- create_matrix(train_tweets[,1], language = "english", removeStopwords = T, stemWords = F,tm::weightTfIdf)
mat <- as.matrix(matrix)
classifier <- naiveBayes(mat, as.factor(train_tweets[,2]))
predicted <- predict(classifier,tweets_new_clean[1:500])
predicted
table(groups)
groups <- cutree(tweets.fit, k=)
groups <- cutree(tweets.fit, k=5)
table(groups)
groups.1 <- groups[groups==1]
table(groups.1)
groups <- cutree(tweets.fit, k=5)
train_tweets <- data.frame(tweets = tweets_clean[1:2000,], cluster = groups)
tweets_new_clean <- apply(as.data.frame(tweets_new_ht), 1, clean_sub)
matrix <- create_matrix(train_tweets[,1], language = "english", removeStopwords = T, stemWords = F,tm::weightTfIdf)
mat <- as.matrix(matrix)
classifier <- naiveBayes(mat, as.factor(train_tweets[,2]))
predicted <- predict(classifier,tweets_new_clean[1:1000])
table(predicted)
clean_sub <- function(x){
#remove all punctuations
gsub("[[:punct:]]", "",
#remove all control characters
gsub("[[:cntrl:]]", "",
#remove all digit
gsub("\\d+","",
#remove the hashtag
gsub("#[_a-zA-Z1-9]+", "",
#remove the users
gsub("@([_a-zA-Z1-9]+)","",
#remove the url
gsub("(https?://[^ ]+)","",x))))))
}
tweets_clean <- apply(tweets, 1, clean_sub)
tweets_clean <-as.character(tweets_clean)
tweets_clean <- stripWhitespace(tweets_clean)
tweets_clean <- removeWords(tweets_clean, stopwords())
ps <- polarity(tweets_clean)
ps <- polarity(tweets_clean[1:10000])
length(tweets_clean)
ps <- polarity(tweets_clean[1:1000])
dim(ps$all)
ps$all[1:10,]
ps <- polarity(tweets_clean[1:10000])
ps <- polarity(tweets_clean[1:5000])
dim(ps$all)
ps$all[1:10,]
positive<- ps$all[ps$all[,3]>0]
positive<- ps$all[,3]>0
negative <- ps$all[,3]<0
tweets_clean[1:5000][which(positive)]
ps$all[which(positive)]
tweets_clean[1:5000][which(positive)]
tweets_clean[1:5000][which(negative)]
pos.corpus <- TermDocumentMatrix(pos.corpus, control= list(stopwords("en"), removeWords=T, stripWhitespace=T)
)
pos.corpus <- Corpus(pos_new)
positive<- ps$all[,3]>0
negative <- ps$all[,3]<0
pos_new <- tweets_clean[1:5000][which(positive)]
neg_new <- tweets_clean[1:5000][which(negative)]
pos.corpus <- Corpus(pos_new)
pos.corpus <- TermDocumentMatrix(pos.corpus, control= list(stopwords("en"), removeWords=T, stripWhitespace=T))
pos.corpus <- Corpus(VectorSource(pos_new))
pos.corpus <- TermDocumentMatrix(pos.corpus, control= list(stopwords("en"), removeWords=T, stripWhitespace=T))
pos.tdm <- TermDocumentMatrix(pos.corpus, control= list(stopwords("en"), removeWords=T, stripWhitespace=T))
pos.corpus <- Corpus(VectorSource(pos_new))
pos.tdm <- TermDocumentMatrix(pos.corpus, control= list(stopwords("en"), removeWords=T, stripWhitespace=T))
inspect(pos.tdm)
pos_freq <- rowSums(pos.tdm)
pos.matrix <- as.matrix(pos.tdm)
pos_freq <- rowSums(pos.tdm)
dim(pos.matrix)
pos.matrix <- as.matrix(pos.tdm)
pos_freq <- rowSums(pos.matrix)
sort(pos_feq)
sort(pos_freq)
sort(pos_freq, decreasing= T)
pos_word_sort <- sort(pos_freq, decreasing= T)
negative <- ps$all[,3]<0
neg_new <- tweets_clean[1:5000][which(negative)]
neg.corpus <- Corpus(VectorSource(neg_new))
neg.tdm <- TermDocumentMatrix(neg.corpus, control= list(stopwords("en"), removeWords=T, stripWhitespace=T))
neg.matrix <- as.matrix(neg.tdm)
neg_freq <- rowSums(neg.matrix)
neg_word_sort <- sort(neg_freq, decreasing= T)
neg_word_sort
negative <- ps$all[,3]<0
neg_new <- tweets_clean[1:5000][which(negative)]
neg.corpus <- Corpus(VectorSource(neg_new))
neg.tdm <- TermDocumentMatrix(neg.corpus, control= list(stopwords("en"), removeWords=T, stripWhitespace=T))
neg.matrix <- as.matrix(neg.tdm)
neg_freq <- rowSums(neg.matrix)
neg_word_sort <- sort(neg_freq, decreasing= T)
positive.words
pos_word_sort <- sort(pos_freq, decreasing= T)
pos_word_sort
pos_word_sort <- as.vector(sort(pos_freq, decreasing= T))
pos_word_sort
as.vector(pos_word_sort()
)
as.vector(pos_word_sort
)
pos_word_sort <- sort(pos_freq, decreasing= T)
pos_word_sort
positive_words_tweets <- names(pos_word_sort)
positive_words_tweets
pos_word_sort <- names(sort(pos_freq, decreasing= T))
pos_word_sort
pos_word_sort <- names(sort(pos_freq, decreasing= T))
positive_tweets <- as.vector(NULL)
for (word) in pos_word_sort{
if (word %in% positive.words){
positive_tweets <- c(positive_tweets,word)
}
}
for (word in pos_word_sort){
if (word %in% positive.words){
positive_tweets <- c(positive_tweets,word)
}
}
positive_tweets
neg_word_sort <- names(sort(neg_freq, decreasing= T))
negative_tweets <- as.vector(NULL)
for (word in neg_word_sort){
if (word %in% negative.words){
negative_tweets <- c(negative_tweets,word)
}
}
negative_tweets
afinn_list <- read.delim(file="AFINN-111.txt", header=F)
setwd("~/Documents/NEU Bioinformatics/DSCS6030_Intro_Data_Mining:Machine_Learing/Moudle8_Text_Mining/Assignment/M8L3")
afinn_list <- read.delim(file="AFINN-111.txt", header=F)
names(afinn_list) <- c("word", "score")
afinn_list$word <- tolower(afinn_list$word)
positive_afinn <- c(afinn_list$word[affinn_list$score>0])
positive_afinn <- c(afinn_list$word[afinn_list$score>0])
tweets.5000.corpus <- Corpus(VectorSource(tweets_clean[1:5000]))
tweets.5000.tdm <- TermDocumentMatrix(tweets.5000.corpus, control= list(stopwords("en"), removeWords=T, stripWhitespace=T))
tweets.5000.matrix <- as.matrix(tweets.5000.tdm)
tweets.5000.matrix
tweets.5000.freq <- rowSums(tweets.5000.matrix)
tweets.5000.freq
words.5000 <- names(tweets.5000.freq)
words.5000
positive_tweets <- as.vector(NULL)
for (word in words.5000){
positive_tweets <- c(positive_tweets,word)
}
else if (word %in% negative_afinn){
negative_tweets <- c(negative_tweets,word)
}
positive_tweets <- as.vector(NULL)
negative_tweets <- as.vector(NULL)
for (word in words.5000){
if (word %in% positive_afinn){
positive_tweets <- c(positive_tweets,word)
}
else if (word %in% negative_afinn){
negative_tweets <- c(negative_tweets,word)
}
}
afinn_list <- read.delim(file="AFINN-111.txt", header=F)
names(afinn_list) <- c("word", "score")
afinn_list$word <- tolower(afinn_list$word)
positive_afinn <- c(afinn_list$word[afinn_list$score>0])
negative_afinn <- c(afinn_list$word[afinn_list$score<0])
positive_tweets <- as.vector(NULL)
negative_tweets <- as.vector(NULL)
for (word in words.5000){
if (word %in% positive_afinn){
positive_tweets <- c(positive_tweets,word)
}
else if (word %in% negative_afinn){
negative_tweets <- c(negative_tweets,word)
}
}
positive_tweets
negative_tweets
ps$all[1:10,]
pol_frame <- sentiment_frame(positives = positive_tweets, negative=negative_tweets, pos.weights = 1, neg.weights = -1)
ps.new <- polarity(tweets_clean[1:5000])
dim(ps.new$all)
ps.new$all[1:50,]
ps$all[1:50,]
ps.new$all[1:50,]
ps.new <- polarity(tweets_clean[1:5000], polarity.frame = pol_frame)
dim(ps.new$all)
ps.new$all[1:50,]
classifier <- naiveBayes(mat, as.factor(train_tweets[,2]))
predicted <- predict(classifier,tweets_new_clean)
predicted <- predict(classifier,tweets_new_clean[1:10000])
table(predicted)
predicted <- predict(classifier,tweets_new_clean)
predicted <- predict(classifier,tweets_new_clean[1:50000])
library("RTextTools")
library("e1071")
library("qdap")
library("tm")
tweets.url <- "http://nikbearbrown.com/YouTube/MachineLearning/M08/ML.Tweets.csv"
tweets.new.url <- "http://nikbearbrown.com/YouTube/MachineLearning/M08/ML.Tweets.New.csv"
tweets <- read.csv(url(tweets.url), header=F)
tweets.new <- read.csv(url(tweets.new.url), header=F)
